---
title             : "A \"Standard Model\" of Early Language Learning Requires Absolute Measurement"
shorttitle        : "Standard Model Measures"

author: 
  - name          : "George Kachergis"
    affiliation   : "1"
    corresponding : yes 
    address       : "450 Jane Stanford Way, Stanford, CA 94305"
    email         : "kachergis@stanford.edu"
  - name          : "Virginia A. Marchman"
    affiliation   : "1"
    corresponding : no 
    address       : "450 Jane Stanford Way, Stanford, CA 94305"
    email         : "marchman@stanford.edu"
  - name          : "Michael C. Frank"
    affiliation   : "1"
    corresponding : no 
    address       : "450 Jane Stanford Way, Stanford, CA 94305"
    email         : "mcfrank@stanford.edu"

affiliation:
  - id            : "1"
    institution   : "Stanford University"

authornote: |
  Department of Psychology

abstract: |
  A "standard model" which formally unites consensus verbal theories to make quantitative predictions is a critical step for improving theories of early language learning. Although early attempts at such a standard model have been made, their impact has been limited due to a lack of both 1) a
   A number of computational models of early vocabulary learning assume that individual words are learned through an accumulation of environmental input. This assumption is also implicit in empirical work that emphasizes links between home language input and learning outcomes. However, models have typically focused on average performance, while empirical work has focused on variability. To capture individual variability, we relate the tradition of research on accumulator models to Item-Response Theory models from psychometrics. This formal connection also makes it clear that the currently available datasets cannot allow us to fully test these models, illustrating a critical need for theory in shaping new data collection and creating and testing an eventual "standard model." 
  
keywords          : "early language learning; language acquisition; vocabulary development"  
wordcount         : "2921"

bibliography      : ["references.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction




Psychology, in general, has been criticized for lacking such formal theories that inform and drive empirical research [@muthukrishna2019problem]. 
Cumulative theories that are iteratively refined,.

This basic framework corresponds nicely with a group of computational models that has been explored in the word learning literature, which we refer to jointly as "accumulator models" [@mcmurray2007;@hidaka2013;@mollica2017]. 
Our aim is to make an explicit connection between these models and the broader but less formal discussion of the role of language input in learning. 

For an overview of best practices for collecting long format sensory experience (LFSE) recording, see @casillas2019step.  

## Absolute > Relative Measurements


## Measurement Error

accuracy, precision..
If we only binary code child-directed / other-directed at the level of 5-minute segments, then even assuming that human coders are 100% accurate we have potential error of just less than 2.5 minutes per segment.

Relative (Fractional) Uncertainty = | uncertainty / (measured quantity) |
Propagation of uncertainty - do an example for all-day LENA recordings?
<!-- https://www.webassign.net/question_assets/unccolphysmechl1/measurements/manual.html -->
NIST says we should report total combined uncertainty for any measured values.
NIST. Essentials of Expressing Measurement Uncertainty. http://physics.nist.gov/cuu/Uncertainty/
Lichten, William. Data and Error Analysis., 2nd. ed. Prentice Hall: Upper Saddle River, NJ, 1999.

## Reliability

How reliable are human coders, e.g. at distinguishing child-directed from other-directed speech?
How reliable are measures of children's speech environment (e.g., adult word count per hour), and how do they relate to the duration of the recording?

A variety of reliability coefficients have been devised, which generally range from 0 to 1, and are meant to quantify the proportion of variation in a test score that is due to differences in the true scores of the test takers, and how much is due to measurement error or noise. 

Classical Test Theory / True Score Theory: an individual's observed score on a test is X = T + E, where T is their 'true' score and E is measurement error.
Unfortunately, we only have access to X, and can only estimate the other values.

```{r, out.width="0.95\\linewidth", include=TRUE, fig.align="center", fig.cap=c("A schematic of the standard relationships between variables assumed in the literature on early language learning. It is generally assumed that child-directed speech (red) is more valuable than overheard speech (blue)."), echo=FALSE}
knitr::include_graphics("./figs/fig1_standard_model.pdf")
```


# Accumulator Models Are An Important Formalism For Describing Word Learning

We are interested in children’s vocabulary, which we define as the set of words they know.^[
What makes a word is its own separate and difficult question, especially as we look across languages. 
Are "dog" and "dogs" two separate words? 
If not, what about "mouse" and "mice"? 
These questions are relatively inconsequential in morphologically simple languages like English and Mandarin but much more difficult in morphologically complex languages like Turkish or agglutinating languages like Inuktitut. 
Yet the Turkish or Inuktitut learner is accumulating *something*. 
Whether or not we call it a word *per se*, our hypothesis is that this unit is being accumulated and that its accumulation will be subject to many of the same dynamics as words.]
Driven by influential observations of the strong relationship between language input and children’s word learning [e.g., @huttenlocher1991early; @hart1995], computational modelers have created a range of hypotheses about how linguistic experiences drive learning. 
Accumulator models, which we focus on here as the foundation for a potential "standard model", assume that linguistic experiences with words accumulate in separate registers (depicted as "buckets" in Figure 2), and that those registers that exceed a particular threshold are learned. 

Accumulator models have already contributed significantly to several theoretical issues. 
For example, @mcmurray2007 elegantly demonstrated that children’s "vocabulary explosion" -- an acceleration in vocabulary growth in the second year -- can result from the steady accumulation of word tokens without changes in the environment or learning mechanism. 
Other work has examined similar models using more realistic distributions of words, developmental change in learning mechanisms, and comparison to children’s aggregate vocabulary growth [@mitchell2009; @hidaka2013; @mollica2017]. 
Although this work is exciting, these models have not yet made direct predictions about learning in individual children [@bergelson2020comprehension]. 

One difficulty in connecting such models to data is that the relevant variables are often measured in relative, rather than absolute, units. 
This practice is common. 
For example, measures of intelligence are given on a standardized scale defined by population variability. 
Yet in early language -- unusually in a psychological domain -- we have access to absolute units.
We can count how many words a child hears and express this number as a rate [e.g., words per hour; @cristia2020accuracy]. 
We can similarly estimate how many words they know [e.g., by parents' reports of vocabulary size; @frank2021]. 
These absolute units give the potential for a model to make powerfully general quantitative predictions, which could be tested across different situations and populations. 
Few datasets have measurements of the relevant variables, however. 
Thus, one upshot of our argument here is that future data collection efforts should attempt whenever possible to report measurements in absolute units. 

```{r, out.width="0.7\\linewidth", include=TRUE, fig.align="center", fig.cap=c("An illustration of an accumulator model: each bucket represents the child’s knowledge about a particular word, and each token is a drop in the corresponding bucket. Some words are more difficult than others (i.e., have larger buckets). When a bucket is full, the corresponding word is considered to be learned. In theory, language input coming from child-directed speech (red) may count more than tokens in overheard speech (blue)."), echo=FALSE}
knitr::include_graphics("./figs/fig2_standard_model.pdf")
```

# Accumulator Models are Presupposed in the Empirical Literature

Since seminal work by @hart1995, the connection between children’s language input and the growth of vocabulary has been a topic of intense interest. 
Numerous studies have reported positive associations between these variables [e.g., @hoff2003specificity] -- as predicted by accumulator models -- though their magnitude varies across studies [@wang2020meta].
Such correlations are also partially moderated by other factors, including socioeconomic [@hoff2003specificity] and genetic [@hayiou2014language] variables.
Randomized interventions are the gold standard for measuring causal effects of parental language input on language learning, although such studies are costly and difficult to conduct. 
When such studies are conducted, they show modest but reliable effects [e.g., @suskind2016project], providing support for a causal connection between language input and outcomes. 
Critically, however, these studies only estimate the effect of the change in input on variation in outcome, hence, providing only a relative, rather than an absolute, estimate of how important input is to vocabulary learning.  

Reasoning from first principles, you cannot learn the word "table" if you do not hear it: input must predict learning of individual words. 
Yet correlational studies do not assess this word-level relation fully, and instead assess whether variation in input relates to variation in learning across children. 
However, a second line of work focuses on differences between words, not children. 
These studies use regression models to predict which words are easier or more difficult, averaging across children [e.g., @goodman2008does; @braginsky2019]. 
They typically show a strong association between word frequency and the age at which children acquire particular words on average, especially for object labels. 
This general finding provides convergent support for accumulator models, absent the confound of differences between individuals.
In sum, accumulator models provide the conceptual underpinning of the relationships between input (frequency for words, quantity for children) and learning. 

# Connecting Accumulator Models with Psychometric Models

The core of the view that we are describing is that language learning is a process of accumulation. 
Individual experiences with words lead to their eventual acquisition. 
The more of these experiences a child receives, the faster their vocabulary grows. 
However, both children and words vary: children may learn faster or slower, and words can be more or less difficult to learn. 
Combining these ideas, the basic hypothesis is that a child’s vocabulary at a particular time should be predicted by their cumulative language exposure and learning rate, combined with the breadth of the sample of words to which they have been exposed and those words' individual difficulties. 

This hypothesis describes an approach similar to Item-Response Theory [IRT; @embretson2013item], an influential and popular psychometric modeling framework. 
IRT is commonly used for estimating the ability of "test takers" (in our case language learners) as they are assessed with a particular set of "test items" (particular words). 
IRT models provide a convenient and broadly-used framework within which to describe and compare different model variants, which in turn represent different sets of theoretical assumptions.

In their formal structure, IRT models describe individual item responses as a function of both the difficulty of specific words and the language abilities of individual children. 
These latent parameters can be inferred from an observed dataset. 
In the basic Rasch (or 1-parameter logistic) IRT model, a person $i$ responds correctly to item $j$ with probability determined by their ability ($\theta_i$) and the difficulty of item $j$ ($d_j$):

$$ P(y_{i,j} = 1 | \theta_i, d_j) = \frac{1}{1+e^{\theta_i + d_j}} $$

These parameters can easily be mapped onto the accumulator model we have been describing: items are words (e.g. $d_j$ is the difficulty of word $j$), and $\theta_i$ is child $i$’s estimated latent language ability. 

In typical IRT models, both item difficulties and person abilities are standardized (assumed to be normally-distributed and centered on 0) and unit-free. 
However, in principle, it is possible to map these scores to real-world distributions incorporating measured word frequencies and rates of children’s experienced input. 
With this mapping into absolute units, this kind of model can provide a quantitative linking hypothesis between measurements of input and learning. 

We can then use standard extensions of IRT models to explore different assumptions. For example, we can include both item-level covariates -- e.g., estimates of word frequency or lexical class [@braginsky2019] -- and person-level covariates -- e.g., sex, socioeconomic status variables, or other demographic information. 
We can also consider whether language ability is uni-dimensionality or whether a multi-factor model is necessary [@frank2021]. 
A final benefit of the IRT framework is that we can use a standardized set of tools for comparison of models on their fit to data, a major benefit over more ad-hoc frameworks. 

# Comparing Model Variants as a Method for Evaluating Theories

We next give an example of how IRT accumulator models can be fit to data and hence how different theoretical assumptions can be compared empirically in the framework described above. 
We ask, is a 2-year-old different than a 1-year-old, aside from having twice as much experience with each word? 
That is, is the amount of language experience the only developmental change we predict (as assumed by the simplest version of an accumulator model)? 
Or are there other age-related changes that distinguish younger and older children's learning?

To answer this question, we fit IRT models to parent report data about the words that children produce.
In the simple setting we describe, these models can be written as multi-level logistic regression models [@de-boeck2011], albeit with a clearer theoretical interpretation than typical regression models. 
We downloaded data from 5,429 monolingual, English-learning children between 16 and 30 months from Wordbank [@frank2021]. 
These data were collected using the MacArthur-Bates Communicative Development Inventory (CDI), a reliable and valid parent report instrument; they include 680 binary responses per child about whether a particular child produced a particular word at a given age. 
Each model estimated parameters for each child's linguistic ability and each word's difficulty.

We also included the frequency of each word as the key parameter controlling its accumulation, treating frequency as an item-level covariate of difficulty.
We do not have access to word frequencies in individual children's environments, however. 
Instead, we estimated average word frequencies using the American English corpora in CHILDES [@macwhinney2000childes] retrieved from childes-db [@sanchez2019childes], following previous work [@goodman2008does;@braginsky2019].
<!-- need to specify age cut-off? ToDo GK - believe it was all data -->
We estimated that an average child receives 1,200 words/hour, 12 hours/day, for a total of 438,300 tokens/month, of which 285,200 tokens (65.1%) are tokens of words appearing on the CDI (see Appendix). 
<!-- HOW WAS THIS ^^ COMPUTED! NEED AN APPENDIX OR NOTES IN THE REPO OR SOMETHING -->
We then included frequency estimates for each word of the tokens per month heard by a typical child (range: 0.2 - 19,286 tokens). 

In our simple accumulator model, we used as our covariate for each child the expected total tokens they had heard given their age. 
We also included an interaction between frequency and lexical category, as previous work has suggested that frequency is more predictive for nouns than for other word classes [@goodman2008does;@braginsky2019].
In contrast, in our more complex model, we included age as a person-level covariate, allowing for interactions between the child's age and the frequency and lexical class predictors.

The more complex model showed a better fit to the data and were preferred over under several model selection metrics (AIC and BIC).
Predicted acquisition curves from this model are shown in Figure 3 by age, lexical class, and prevalence (left), revealing that nouns are learned more rapidly than verbs or function words, and that more frequent nouns are learned earlier, but there is little effect of frequency for verbs and function words. 
Moreover, the model predicts item-level acquisition curves (Figure 3, right): e.g., "ball" is learned earlier than "dog" and "go" is easier than "have." 
Further, its better fit provides support for age-related changes in the accumulation mechanism. 
More generally, this simulation demonstrates the possibility of using large-scale datasets to test hypotheses about the nature of learning mechanisms. 

```{r, out.width="1.0\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Predicted acquisition curves derived from a fitted age-dependent accumulator model (see text) as a function of lexical class and the number of expected tokens per month (left), and for a sample of specific (right)."), echo=FALSE}
knitr::include_graphics("./figs/fig3_predicted_age_LC_and_items.pdf")
```

# Onward Towards a Standard Model

The goal of any computational theory is to derive the predictions arising from a specific set of assumptions. 
Often, however, it is the *failure* of such a model to predict observed patterns of data that is most useful, as these failures point the way forward towards future refinements [@vanrooij2020theory]. 
We discuss three potential failures of simple accumulator models. 

**Leveraged learning and the role of processing**. 
As our results above show, older children accumulate language from their input faster than younger children.
Why?
Are older children simply better at remembering words, leading to "bigger drops" in their buckets? 
Or do older children *leverage* their knowledge of language to learn faster than younger children [@mitchell2009]? 
They could do this by reasoning about new words by exclusion [@markman1988children]. 
Or could their increasing fluency with the words they know help them learn new words? 
A surprising proportion of variance in the rate of young children’s vocabulary growth is accounted for by the speed with which they process familiar words [e.g., @marchman2016early].
These theoretical proposals about which words should facilitate the learning of other words yield predictions that could be tested using the models we describe.

**A theory for understanding acquisition in diverse contexts**. 
The framework above formalizes an implicit assumption: namely, children learn the same way in all circumstances.
Yet this assumption could very well be false. 
For example, some children might learn more from overheard speech and others might learn more from child-directed speech [@sperry2019]. 
The kind of data necessary to test this assumption directly are only now being collected, for example, in studies that rigorously track learning outcomes and amount of language input in diverse populations [e.g., comparisons between children in low-income, rural, indigenous communities and those in higher-income Western contexts; @casillas2020].

**Beyond vocabulary**. 
We have described a model of the accumulation of words.
Yet language is a rich, complex system in which words are inflected morphologically and composed syntactically to express compositional meanings. 
In early views, syntax and morphology were conceptualized as distinct and unconnected, but this conception has not been borne out empirically. 
Instead, evidence shows again and again that the language system is "tightly woven," with extremely tight correlations between the acquisition of words, morphology, and syntax [@frank2021]. 
Theoretically, accumulator models like the standard model are generic models of skill acquisition. 
If language learning is a form of skill acquisition [@chater2018language], such connections could lead the way towards extensions of the standard model to the accumulation of broader units of language-like constructions. 

# Conclusions

An implicit theory drives much research and policy-making on early language acquisition: early language accumulates through discrete experiences with individual words. 
The more experiences, the faster the words are learned. 
This implicit theory can be expressed within a family of computational models that make quantitative predictions. 
By situating these models in a common psychometric framework, we show how they can be used to connect to large-scale, cross-linguistic datasets. 
This modeling framework synthesizes measures of language input and vocabulary growth, allowing us to formalize, test, and iteratively improve our understanding.
Moreover, this formalization allows us to identify specific gaps in current empirical approaches that must be closed to inform future theory development.
Perhaps one day soon, these developments will lead to a true "standard model" of language learning.

\newpage

# References
```{r create_r-references}
#r_refs(file = "references.bib") # 31 refs right now
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
